---
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Analysis Protocols {.tabset .tabset-pills .tabset-fade}

Once you have generated the data from the MiSeq you must complete the following steps in order to analyze the data.


## Software Requirements

The primary software used for analysis is Mothur. This program is command line based, and will require you to type simple commands into either Terminal (Mac) or Command Prompt (Windows).

To download Mothur go to the Mothur github [downloads](https://github.com/mothur/mothur/releases/latest) page and download the latest version for your operating system. Unzip then move this folder to your home directory (your C: Drive on a PC, or your ‘User’ folder on a Mac).

<div class="panel panel-info">
<div class="panel-heading">Tip</div>
<div class="panel-body">

To check that you have placed the folder in the correct location, open up either Terminal (Mac) or Command Prompt (PC). Type `~/mothur/mothur` (Mac) or `C:\mothur\mothur.exe` (PC). If your screen looks similar to this:

![](images/mothur_screenshot.png)

You have done it correctly.

</div>
</div>

## Preparing files for analysis

### Downloading sequencing data files from BaseSpace

To download the sequencing fields from BaseSpace. Go to https://basespace.illumina.com/home/index, log-in, and navigate to your sequencing run. Click “Download” and select “All FASTQ files for this run”. Then click “Download your files”. BaseSpace uses the “BaseSpace Downloader” to download your files. If you have not previously downloaded this program, you must download it at this time. Select the location to download your files.

Once your files have downloaded, navigate to the folder where the files were downloaded. There will be one folder that corresponds to each sample that was sequenced. In each of these folders there will be two files, one that corresponds to the forward reads, and one that corresponds to the reverse reads. The forward and reverse read files for all samples will need to be moved to a single folder for analysis to conduct the analysis. 

<div class="panel panel-success">
<div class="panel-heading">Recommendation</div>
<div class="panel-body">

Create a folder on your home directory with the name of your sequencing run (for the rest of this example it will be called “MiSeq_Run”). Move all of your files into this folder.

</div>
</div>

Once the files have been moved, the files must be unzipped/uncompressed. This can be done with the Archive Utility on a Mac and any file compression software, such as WinZip, on a PC. Make sure you unzip your files into the folder that you have created (i.e. “Miseq_Run”)

### Create sample list

You will need to create a file that contains the name of your sample, and specifies which files correspond to this sample. This will tell the program which files to analyze for each particular sample. You can download the file “nemabiome.files” in the [Examples](interpretation.html) section for an example of what this file must look like. Each line in this file represents a single sample that must be analyzed. The first column is the sample name, the second column is the file name for the forward read file, and the third column is the name of the reverse read file. Each column is separated by a single tab.   An example is also listed below.

Example:

```
sample1	sample1_S1_L001_R1_001.fastq	 sample1_S1_L001_R2_001.fastq
sample2	sample2_S2_L001_R1_001.fastq	 sample2_S2_L001_R2_001.fastq
sample3	sample3_S3_L001_R1_001.fastq	 sample3_S3_L001_R2_001.fastq
...
```

Sample names may vary between different Illumina platforms, so please check the names of your downloaded files, to ensure the file names are correct. Please name this file as “stability.files”, with the “.files” file extension. Place this file in your folder that you created for analysis. This is the only file that you will need to create from scratch for this analysis, if you use the provided files.

### Download analysis files

Download the analysis files <a href="downloads/nematode_ITS2_database_version1_3.fasta" download>Nematode ITS2 database version 1.3</a>,  <a href="downloads/nematode_taxonomy_1_3.tax" download>Nematode taxonomy file (version 1.3)</a> and <a href="downloads/batchfile.txt" download>Mothur batch file</a>. Place these files in the folder that contains your sequencing data (i.e. 'MiSeq_Run')

For a standard application do not make any changes to these files. If you are aware of the number of processors that your computer has, open up the `batchfile.txt` file, go to the first line and change the parameter “processors” from “processors=2” to the number of processors in your computer. 

## Running analysis

Once all files have been downloaded, open up Terminal or Command Prompt. You will then need to navigate to the folder you created, which can be done using the “cd” command. If you created your analysis folder in your home directory type:

```
cd MiSeq_Run
```

This will change to this folder.

Then type:

```
~/mothur/mothur batchfile.txt
```  
(On a Mac)
 
OR

```
C:\mothur\mothur.exe C:\MiSeq_Run\batchfile.txt
``` 
(On a PC)

This will launch Mothur, and begin to complete the commands that are contained within the batchfile. You will need to leave your computer open and running while the analysis completes. The amount of time that it will take to run will vary with your computer, and the number of samples/reads that you will be analyzing. General run times will probably be around ~4 hours, on a standard computer, 96 samples with an average read depth of ~50,000 reads.


Once the analysis has completed, you will have a file called `nemabiome_results.summary`. You can open this file up and copy the contents into an Excel workbook to properly view the data. 

## DADA2 Example Workflow

The following is a sample analysis using the DADA2 pipeline (https://benjjneb.github.io/dada2/tutorial.html). This is meant to be an example and not a definitive workflow. First we load packages and set a seed for reproducibility.

```{r libraries, echo=T, results='hide'}
library(DECIPHER)
packageVersion("DECIPHER")

library(dada2)
packageVersion("dada2") 

library(ShortRead)
packageVersion("ShortRead") 

library(Biostrings)
packageVersion("Biostrings")

library(ggplot2)
packageVersion("ggplot2")

set.seed(106)
```

To use the DADA2 pipeline, primers must be removed from amplicons. This can be done in R using cutadapt. Here we specify the path of sequences to be processed (using "/Miseq_Run" from previous examples) and the path of Cutadapt on your machine (we used "/home/gilleardlab/anaconda3/bin/cutadapt"). You will also need to specify the primers you used as the "FWD" and "REV" variables. We start off by creating all possible orientations of the forward and reverse sequences.

```{r cutadapt_initialization}
path<- "~/MiSeq_Run/"
list.files(path)

fnFs<-sort(list.files(path, pattern = "_R1_001.fastq.gz", full.names = TRUE)) 
fnRs <-sort(list.files(path, pattern = "_R2_001.fastq.gz", full.names = TRUE))


#identifying the primers
FWD<-"ACGTCTGGTTCAGGGTTGTT"
REV <- "TTAGTTTCTTTTCCTCCGCT"

allOrients<-function(primer) {   #creates all orientations of the primer sequence)
  require(Biostrings)
  dna<-DNAString(primer) #biostring works w dnastring objects
  orients<-c(Forward=dna, Complement=complement(dna), Reverse=reverse(dna),
             RevComp=reverseComplement(dna))
  return(sapply(orients, toString)) #now converting back into character vector
}
FWD.orients<-allOrients(FWD)
REV.orients<-allOrients(REV)

FWD.orients
```

Next we will remove all sequences containing ambiguous bases, and count the number of times our primers are observed in the remaining sequences.

```{r more_cutadapt_initialization}
#fltering the sequences
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) 
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

#counting number of primers 
primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
```

Now we can use cutadapt to remove primers, and check that they have been successfully removed.

```{r cutadapt, echo=T, results='hide'}
cutadapt <- "/home/stefan/.local/bin/cutadapt" # CHANGE ME to the cutadapt path on your machine
system2(cutadapt, args = "--version") # Run shell commands from R

#now creating output filenames for cutadapted files, and define parameters to give cutadapt command
#critical parameters are the primers and they need to be in the right orientation

path.cut <- file.path(path, "cutadapt")
if(!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))  #cutadapt not working -> due to files not gz?

FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
# Trim FWD and the reverse-complement of REV off of R1 (forward reads)
R1.flags <- paste("-g", FWD, "-a", REV.RC) 
# Trim REV and the reverse-complement of FWD off of R2 (reverse reads)
R2.flags <- paste("-G", REV, "-A", FWD.RC) 
# Run Cutadapt
for(i in seq_along(fnFs)) {
  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, # -n 2 required to remove FWD and REV from reads
                             "-o", fnFs.cut[i], "-p", fnRs.cut[i], # output files
                             fnFs.filtN[i], fnRs.filtN[i])) # input files
}

# remember to check things - can check presence of primers in the first cutadapted sample 
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))

```

Now that primers are removed, we can begin the DADA2 pipeline. We start with plotting quality profiles of our sequences to determine suitable quality filtering parameters. 

```{r quality_profiles}
cutFs <- sort(list.files(path.cut, pattern = "_R1_001.fastq.gz", full.names = TRUE)) #remember to match file names 
cutRs <- sort(list.files(path.cut, pattern = "_R2_001.fastq.gz", full.names = TRUE))

# Extract sample names, assuming filenames have same format, may need to alter!!!
get.sample.name <- function(fname) strsplit(basename(fname), "_R")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
sample.names

#plotQualityProfile(cutFs[1:2])+ggtitle("Forward")
#plotQualityProfile(cutRs[1:2])+ggtitle("Reverse")
```
For the purposes of our analysis, we chose to conduct quality filtering with maximum expected errors of 2 for the forward sequence, 5 for the reverse, to truncate after a quality score of 2 or lower, and to filter out amplicons with lengths shorter than 50 base pairs. Again, filters for your own pipeline may differ. After quality filtering, we can run our sequences through learnErrors to create an error model, and then dereplicate our sequences.

```{r}
#filtering reads
filtFs<-file.path(path.cut, "filtered", basename(cutFs))
filtRs<-file.path(path.cut, "filtered", basename(cutRs))

out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 5), 
                     truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  
head(out)
out

#learning errors
errF<-learnErrors(filtFs, multithread=TRUE)
errR<-learnErrors(filtRs, multithread=TRUE)

#visualize the error rates w code below so you can see
plotErrors(errF, nominalQ=TRUE)

# denoising data 
derepFs<-derepFastq(filtFs, verbose = TRUE)
derepRs<-derepFastq(filtRs, verbose=TRUE)
```

After dereplication, we can denoise our sequences using the DADA algorithm. We will also merge our paired-end sequences, allowing for up to 1 mismatch in the overlap region and returning all sequences whether they merged or not.

```{r denoise_and_merge, echo=T, results='hide'}
dadaFs<-dada(derepFs, err=errF, multithread = TRUE)
dadaRs<-dada(derepRs, err=errR, multithread=TRUE)

mergers<-mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE, maxMismatch = 1, returnRejects = TRUE)
```

Before we can assign taxonomy, we can construct a sequence table, remove chimeras using the removeBimeraDenovo command and then create a table of how many sequences were retained after each processing step.

```{r chimera_removal}
seqtab<-makeSequenceTable(mergers)
dim(seqtab) 

#Remove chimeras 
seqtab.nochim<-removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
#inspect distribution sequence lengths
table(nchar(getSequences(seqtab.nochim)))

#track reads through the pipeline - inspect the number of reads that made it through each step in the pipeline -- IMPORTANT

getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, 
                                                                       getN), rowSums(seqtab.nochim))

# If processing a single sample, remove the sapply calls: e.g. replace
# sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", 
                     "nonchim")
rownames(track) <- sample.names     #what's considered to be an overlarge drop?? - reads seem fine
head(track)
```

## Download list {#downloads}

* <a href="downloads/batchfile.txt" download="downloads/batchfile.txt">Mothur batch file</a>
* <a href="downloads/nematode_taxonomy_1_3.tax" download>Nematode taxonomy file (version 1.3)</a>
* <a href="downloads/nematode_ITS2_database_version1_3.fasta" download>Nematode ITS2 database version 1.3</a>
